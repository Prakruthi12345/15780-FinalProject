{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_highway = \"trajectories-0400-0415.csv\" #highway\n",
    "csv_intersection=\"NGSIM__Lankershim_Vehicle_Trajectories.csv\" #intersection\n",
    "\n",
    "chunk_size = 100000 \n",
    "dataframes = []\n",
    "for chunk in pd.read_csv(csv_intersection, chunksize=chunk_size):\n",
    "    dataframes.append(chunk)\n",
    "\n",
    "#Vehicle ID is not unique, but as per data Vehicle ID and Total Frames together is primary key\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df['Unique_ID'] = df['Vehicle_ID'].astype(str) + df['Total_Frames'].astype(str)\n",
    "\n",
    "\n",
    "dataframes_dict = {}\n",
    "\n",
    "\n",
    "grouped = df.groupby('Unique_ID')\n",
    "\n",
    "#Grouped data by each unique ID\n",
    "for group_name, group_df in grouped:\n",
    "    dataframes_dict[group_name] = group_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Transformer Architecture_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, seq_len, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        \n",
    "        self.positional_encoding = self._generate_positional_encoding(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # predicting two values: v_Vel and v_Acc\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model))\n",
    "        pe = self._generate_positional_encoding(src.size(0))  # generate positional encoding dynamically\n",
    "        src += pe\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.output_layer(output[:, -1, :])\n",
    "        return output\n",
    "    \n",
    "    def _generate_positional_encoding(self, batch_size):\n",
    "        position = torch.arange(0, self.seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / self.d_model))\n",
    "        pe = torch.zeros(self.seq_len, self.d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        return pe\n",
    "\n",
    "input_dim = 3  \n",
    "nhead = 8  \n",
    "num_encoder_layers = 6  \n",
    "dim_feedforward = 2048 \n",
    "dropout = 0.1  \n",
    "sequence_length=10\n",
    "\n",
    "d_model = 8 * nhead\n",
    "\n",
    "model_transformer = TimeSeriesTransformer(\n",
    "    input_dim=input_dim,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    seq_len=sequence_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*LSTM Architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "input_size=input_dim\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model_lstm = LSTM(input_size, hidden_size, num_layers, output_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Model Training Phase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, target_data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(input_data) - sequence_length):\n",
    "        seq = input_data[i:i+sequence_length]  # Sequence of features\n",
    "        target = target_data[i+sequence_length]  # Next step's features\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "losses_val=[]\n",
    "model=model_transformer #Choose which model to train\n",
    "p=0\n",
    "for Car_ID in dataframes_dict.keys():\n",
    "\n",
    "    df2=dataframes_dict[Car_ID]\n",
    "    if df.empty:\n",
    "        print(Car_ID,\" Empty\")\n",
    "    else:\n",
    "        print(\"Going\")\n",
    "    df2=df2[['Local_X','Local_Y','Lane_ID','Preceding','Following','v_Vel']]\n",
    "    normalized_data = (df2 - df2.mean()) / df2.std()\n",
    "    feature_columns = ['Local_X', 'Local_Y', 'v_Vel']\n",
    "    target_columns = ['v_Vel']\n",
    "\n",
    "    # Create sequences\n",
    "    sequence_length = 10  # For example, using the last 10 timesteps to predict the next\n",
    "\n",
    "   \n",
    "    feature_data = normalized_data[feature_columns].values\n",
    "    target_data = normalized_data[target_columns].values\n",
    "\n",
    "    sequences, targets = create_sequences(feature_data, target_data, sequence_length)\n",
    "\n",
    " \n",
    "    train_size = int(len(sequences) * 0.9)\n",
    "    val_size = int(len(sequences) * 0.1)\n",
    "\n",
    "    train_sequences, train_targets = sequences[:train_size], targets[:train_size]\n",
    "    val_sequences, val_targets = sequences[train_size:train_size+val_size], targets[train_size:train_size+val_size]\n",
    "    test_sequences, test_targets = sequences[train_size+val_size:], targets[train_size+val_size:]\n",
    "\n",
    "\n",
    "    train_sequences = torch.tensor(train_sequences, dtype=torch.float16)\n",
    "    train_targets = torch.tensor(train_targets, dtype=torch.float16)\n",
    "    val_sequences = torch.tensor(val_sequences, dtype=torch.float16)\n",
    "    val_targets = torch.tensor(val_targets, dtype=torch.float16)\n",
    "    test_sequences = torch.tensor(test_sequences, dtype=torch.float16)\n",
    "    test_targets = torch.tensor(test_targets, dtype=torch.float16)\n",
    "\n",
    "    loss_function = torch.nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Adam optimizer\n",
    "\n",
    "\n",
    "    train_data = TensorDataset(train_sequences, train_targets)\n",
    "    val_data = TensorDataset(val_sequences, val_targets)\n",
    "\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    num_epochs = 1 \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        total_iterations = len(train_loader)\n",
    "        for batch_idx, (seq, target) in enumerate(progress_bar):\n",
    "            optimizer.zero_grad()  \n",
    "            seq = seq.float()\n",
    "            target = target.float()\n",
    "            output = model(seq) \n",
    "            loss = loss_function(output, target)  \n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            train_losses.append(loss.item())  \n",
    "\n",
    "            remaining_iterations = len(train_loader) - len(train_losses)\n",
    "            progress_bar.set_postfix(Loss=loss.item(), Remaining=remaining_iterations)\n",
    "\n",
    "        model.eval()  \n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "            for seq, target in val_loader:\n",
    "                seq = seq.float()\n",
    "                target = target.float()\n",
    "                output = model(seq) \n",
    "                loss = loss_function(output, target) \n",
    "                val_losses.append(loss.item()) \n",
    "        losses_val.append(val_losses)\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {np.mean(train_losses):.4f}, Validation Loss: {np.mean(val_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_single.pth') #Save weights for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_transformer\n",
    "model.load_state_dict(torch.load('model_weights_single.pth')) #Use pre-trained weights for test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST ON ANY DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"trajectories-0820am-0835am.csv\"\n",
    "batch_size = 64\n",
    "\n",
    "chunk_size = 100000 \n",
    "dataframes = []\n",
    "for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):\n",
    "    dataframes.append(chunk)\n",
    "\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "df['Unique_ID'] = df['Vehicle_ID'].astype(str) + df['Total_Frames'].astype(str)\n",
    "\n",
    "dataframes_dict = {}\n",
    "\n",
    "grouped = df.groupby('Unique_ID')\n",
    "\n",
    "for group_name, group_df in grouped:\n",
    "    dataframes_dict[group_name] = group_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Car_ID='7031523' #Choose any Unique ID\n",
    "df2=dataframes_dict[Car_ID]\n",
    "df2=df2[['Local_X','Local_Y','Lane_ID','Preceding','Following','v_Vel']]\n",
    "normalized_data = (df2 - df2.mean()) / df2.std()\n",
    "feature_columns = ['Local_X', 'Local_Y', 'v_Vel']\n",
    "target_columns = ['v_Vel']\n",
    "\n",
    "\n",
    "sequence_length = 10  \n",
    "\n",
    "feature_data = normalized_data[feature_columns].values\n",
    "target_data = normalized_data[target_columns].values\n",
    "df3=dataframes_dict[Car_ID]\n",
    "df3=df3[['Local_X','Local_Y','Lane_ID','Preceding','Following','v_Vel']]\n",
    "sequences, targets = create_sequences(feature_data, target_data, sequence_length)\n",
    "sequences = torch.tensor(sequences, dtype=torch.float16)\n",
    "targets = torch.tensor(targets, dtype=torch.float16)\n",
    "vehicle_data = TensorDataset(sequences, targets)\n",
    "vehicle_loader = DataLoader(vehicle_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss_function = torch.nn.MSELoss()  # Mean Squared Error Loss for regression tasks\n",
    "\n",
    "# Evaluate on test data\n",
    "def evaluate(model, test_loader, loss_function):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_losses = []\n",
    "    with torch.no_grad():  # No gradients required for testing\n",
    "        for seq, target in test_loader:\n",
    "            seq, target = seq.float(), target.float()\n",
    "            output = model(seq)\n",
    "            loss = loss_function(output, target)\n",
    "            test_losses.append(loss.item())\n",
    "    return np.mean(test_losses)\n",
    "\n",
    "test_loss = evaluate(model, vehicle_loader, loss_function)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals, predictions = [], []\n",
    "\n",
    "# Function to predict and visualize the results\n",
    "def plot_predictions(model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq, target in loader:\n",
    "            seq, target = seq.float(), target.float()\n",
    "            output = model(seq)\n",
    "            actuals.extend(target.numpy())\n",
    "            predictions.extend(output.numpy())\n",
    "\n",
    "    # Plotting the first few sequences\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(actuals, label='Actual')\n",
    "    plt.plot(predictions, label='Predicted')\n",
    "    plt.title('Comparison of Actual and Predicted Values')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(model, vehicle_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = np.array(actuals)\n",
    "predictions = np.array(predictions)\n",
    "# actual_velocities, actual_accelerations = actuals[:, 0], actuals[:, 1]\n",
    "# predicted_velocities, predicted_accelerations = predictions[:, 0], predictions[:, 1]\n",
    "actual_velocities = actuals[:, 0]\n",
    "predicted_velocities = predictions[:, 0]\n",
    "\n",
    "#De-normalising Predictions\n",
    "actual_velocities=actual_velocities*df2.std()['v_Vel']+df2.mean()['v_Vel']\n",
    "predicted_velocities=predicted_velocities*(df2.std())['v_Vel']+df2.mean()['v_Vel']\n",
    "# print(actual_velocities[0], actual_accelerations[0])\n",
    "# print(predicted_velocities[0], predicted_accelerations[0])\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(10, 10))\n",
    "\n",
    "# Plot actual values\n",
    "axs[0].plot(actual_velocities, label='Actual Velocity', color='blue')\n",
    "axs[0].plot(predicted_velocities, label='Predicted Velocity', color='orange')\n",
    "axs[0].set_title('Comparison of Actual and Predicted Velocities')\n",
    "axs[0].set_xlabel('Time Step')\n",
    "axs[0].set_ylabel('Velocity')\n",
    "axs[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Anomaly Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute differences for velocity\n",
    "velocity_diff = np.abs(actual_velocities - predicted_velocities)\n",
    "\n",
    "# Plotting the box plot for velocity differences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(velocity_diff, vert=False, patch_artist=True)  # Use patch_artist to fill the box with color\n",
    "plt.title('Box Plot of Velocity Differences Between Actual and Predicted Values')\n",
    "plt.xlabel('Difference')\n",
    "\n",
    "# Calculate the IQR and determine the outlier threshold for velocity\n",
    "q1, q3 = np.percentile(velocity_diff, [25, 75])\n",
    "iqr = q3 - q1\n",
    "outlier_threshold = q3 + 1.5 * iqr\n",
    "plt.axvline(x=outlier_threshold, color='r', linestyle='--', label=f'Outlier Threshold at {outlier_threshold:.2f}')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the calculated threshold which can be used for anomaly detection\n",
    "print(\"Suggested anomaly detection threshold:\", outlier_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_velocity = torch.mean(train_targets[:, 0])  # Assuming first target is velocity\n",
    "#std_velocity = torch.std(train_targets[:, 0])\n",
    "\n",
    "# Define a function to detect anomalies\n",
    "def detect_velocity_anomalies(actual_velocity, predicted_velocity, threshold=0.1):\n",
    "    anomalies = []\n",
    "    velocity_diff = np.abs(actual_velocity - predicted_velocity)\n",
    "    for i, diff in enumerate(velocity_diff):\n",
    "        if diff > threshold:\n",
    "            anomalies.append((i, actual_velocity[i], predicted_velocity[i]))\n",
    "    return anomalies\n",
    "\n",
    "# Get anomalies in the predictions\n",
    "velocity_anomalies = detect_velocity_anomalies(actual_velocities, predicted_velocities, threshold=outlier_threshold)\n",
    "print(\"Velocity Anomalies (Index, Actual, Predicted):\", velocity_anomalies)\n",
    "\n",
    "# Calculate the percentage of velocity points that are anomalous\n",
    "velocity_anomalies_percent = (len(velocity_anomalies) / len(actual_velocities)) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Percentage of velocity points that are anomalous: {velocity_anomalies_percent:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if velocity_anomalies:\n",
    "    velocity_anomaly_indices, _, velocity_anomaly_values = zip(*velocity_anomalies)\n",
    "else:\n",
    "    velocity_anomaly_indices, velocity_anomaly_values = [], []\n",
    "\n",
    "# Plot actual + predicted + anomalies\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(actual_velocities, label='Actual Velocity', color='blue')\n",
    "plt.plot(predicted_velocities, label='Predicted Velocity', color='orange')\n",
    "plt.scatter(velocity_anomaly_indices, velocity_anomaly_values, color='red', s=50, label='Anomalies', zorder=5)\n",
    "plt.title(f'Comparison of Actual and Predicted Velocities with Anomalies - Unique ID {Car_ID}')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Velocity')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
